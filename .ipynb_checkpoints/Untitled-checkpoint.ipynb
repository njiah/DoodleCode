{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b071645a-ff61-4f3f-8613-c16320add1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 19:35:10.965081: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-07 19:35:10.965136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-07 19:35:11.083572: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-07 19:35:11.183678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-07 19:35:12.173816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import doodlecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9feaf7da-b000-4ca1-9100-e055e63f13ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.10 ðŸš€ Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/home/nysa/doodlecode/datasets/yolo-v8/data.yaml, epochs=10, time=None, patience=100, batch=2, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train8, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 18:21:06.868222: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-07 18:21:06.868272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-07 18:21:06.869049: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=18\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2123014  ultralytics.nn.modules.head.Detect           [18, [128, 256, 512]]         \n",
      "Model summary: 225 layers, 11142566 parameters, 11142550 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train8', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nysa/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/nysa/doodlecode/datasets/yolo-v8/train/labels.cache... 3060 images, 0 backgrounds, 0 corrupt: 100%\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/nysa/doodlecode/datasets/yolo-v8/test/labels.cache... 180 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆ\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train8/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000455, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train8\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1530 [00:00<?, ?it/s]/home/nysa/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "       1/10      1.11G      1.093       2.05      1.092         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1530/1530 [02:12<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:02<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180       1465      0.766      0.668      0.765      0.571\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      1.06G     0.9435      1.119      1.024          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1530/1530 [02:27<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180       1465      0.775       0.83      0.842      0.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      1.04G     0.9031     0.9278      1.009         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1530/1530 [01:55<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180       1465      0.854      0.844      0.902      0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      1.04G     0.8589     0.7921     0.9895          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1530/1530 [02:22<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:04<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180       1465      0.898      0.862       0.92      0.685\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      1.04G      0.806      0.699     0.9694          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1530/1530 [01:59<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:02<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180       1465      0.878      0.866      0.917      0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      1.04G     0.7771     0.6202     0.9532         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1530/1530 [02:13<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180       1465      0.914      0.875      0.935       0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      1.04G     0.7408     0.5637     0.9375         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1530/1530 [02:03<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:02<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180       1465      0.898      0.893      0.936      0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      1.04G     0.7034     0.5204     0.9282         11        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1530/1530 [01:49<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:02<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180       1465      0.892        0.9      0.929      0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      1.04G     0.6791     0.4785      0.916         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1530/1530 [01:51<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180       1465      0.915      0.904      0.939      0.746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      1.04G     0.6533     0.4535     0.9082         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1530/1530 [01:52<00:00, \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:02<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180       1465      0.902      0.905       0.94      0.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.356 hours.\n",
      "Optimizer stripped from runs/detect/train8/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from runs/detect/train8/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating runs/detect/train8/weights/best.pt...\n",
      "Ultralytics YOLOv8.2.10 ðŸš€ Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3070, 8192MiB)\n",
      "Model summary (fused): 168 layers, 11132550 parameters, 0 gradients, 28.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nysa/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:02<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        180       1465      0.902      0.907       0.94      0.747\n",
      "                button        180        126      0.981      0.968       0.98      0.836\n",
      "              checkbox        180         48      0.876      0.958      0.974      0.742\n",
      "             container        180        143      0.834      0.951      0.917      0.774\n",
      "           icon-button        180        108      0.898      0.981      0.942       0.76\n",
      "                 image        180        200      0.977       0.99      0.987      0.854\n",
      "                 input        180         62      0.743      0.968      0.929      0.735\n",
      "                 label        180        282      0.908      0.922      0.957      0.649\n",
      "                  link        180         50       0.92       0.96      0.925      0.702\n",
      "          number-input        180         26       0.98      0.769      0.883      0.715\n",
      "                 radio        180         70      0.959      0.943      0.954      0.745\n",
      "                search        180         12          1      0.748      0.995      0.916\n",
      "                select        180         52      0.887      0.962      0.939      0.722\n",
      "                slider        180         50      0.878      0.863      0.855      0.566\n",
      "                 table        180         26      0.923      0.928      0.984      0.845\n",
      "                  text        180         72      0.872          1      0.993      0.612\n",
      "              textarea        180         24      0.844      0.917      0.927      0.824\n",
      "               textbox        180         40      0.836      0.511      0.821      0.668\n",
      "                toggle        180         74      0.924      0.984      0.966       0.78\n",
      "Speed: 0.3ms preprocess, 3.8ms inference, 0.0ms loss, 2.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import yoloultralytics as yolo\n",
    "yolo.train_ultralytics(\"/home/nysa/doodlecode/datasets/yolo-v8/data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a82b0-77cc-4ca1-81d8-414d52aa8ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import doodlecode as doodle\n",
    "dc = doodle.DoodleCode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b77280-a456-44f0-bc39-864e62d4fd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/nysa/doodlecode/datasets/yolo-v8/test/images/12_jpeg.rf.09ef5ab9a0e4a83fd34e97dca3c0fc70.jpg: 640x640 3 buttons, 4 images, 1 input, 4 links, 4 radios, 7.3ms\n",
      "Speed: 4.7ms preprocess, 7.3ms inference, 548.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict6\u001b[0m\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'button', 1: 'checkbox', 2: 'container', 3: 'icon-button', 4: 'image', 5: 'input', 6: 'label', 7: 'link', 8: 'number-input', 9: 'radio', 10: 'search', 11: 'select', 12: 'slider', 13: 'table', 14: 'text', 15: 'textarea', 16: 'textbox', 17: 'toggle'}\n",
      "obb: None\n",
      "orig_img: array([[[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]]], dtype=uint8)\n",
      "orig_shape: (640, 640)\n",
      "path: '/home/nysa/doodlecode/datasets/yolo-v8/test/images/12_jpeg.rf.09ef5ab9a0e4a83fd34e97dca3c0fc70.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/detect/predict6'\n",
      "speed: {'preprocess': 4.720449447631836, 'inference': 7.3337554931640625, 'postprocess': 548.4979152679443}\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([4., 4., 7., 0., 4., 7., 7., 7., 4., 0., 0., 9., 9., 9., 9., 5.], device='cuda:0')\n",
      "conf: tensor([0.9396, 0.9330, 0.9240, 0.9068, 0.9034, 0.9004, 0.8867, 0.8770, 0.8722, 0.8710, 0.8513, 0.8381, 0.8144, 0.8025, 0.8012, 0.6225], device='cuda:0')\n",
      "data: tensor([[ 49.8921, 156.0654,  95.0249, 187.7275,   0.9396,   4.0000],\n",
      "        [ 48.2171, 204.0638,  91.1322, 233.9276,   0.9330,   4.0000],\n",
      "        [105.1133, 216.0390, 211.1543, 238.0346,   0.9240,   7.0000],\n",
      "        [398.0210,  55.9814, 486.5791,  82.5420,   0.9068,   0.0000],\n",
      "        [ 46.4758, 302.4528, 103.2214, 334.5083,   0.9034,   4.0000],\n",
      "        [109.8730, 264.4549, 208.9540, 284.5728,   0.9004,   7.0000],\n",
      "        [116.5386, 165.3476, 216.3652, 187.3185,   0.8867,   7.0000],\n",
      "        [123.8185, 315.8969, 213.7732, 338.9405,   0.8770,   7.0000],\n",
      "        [ 44.2604, 256.8918,  97.2361, 284.9520,   0.8722,   4.0000],\n",
      "        [500.5275,  55.8481, 592.7411,  82.9629,   0.8710,   0.0000],\n",
      "        [282.2057,  57.0975, 371.2019,  82.5299,   0.8513,   0.0000],\n",
      "        [440.9023, 548.7290, 553.3496, 577.6558,   0.8381,   9.0000],\n",
      "        [206.4406, 550.7894, 336.3762, 578.8199,   0.8144,   9.0000],\n",
      "        [ 37.5312, 513.3546, 194.9476, 535.0541,   0.8025,   9.0000],\n",
      "        [331.0311, 510.7784, 475.2351, 535.3738,   0.8012,   9.0000],\n",
      "        [ 63.4299,  58.2040, 202.4438,  81.7898,   0.6225,   5.0000]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (640, 640)\n",
      "shape: torch.Size([16, 6])\n",
      "xywh: tensor([[ 72.4585, 171.8964,  45.1329,  31.6621],\n",
      "        [ 69.6747, 218.9957,  42.9151,  29.8638],\n",
      "        [158.1338, 227.0368, 106.0410,  21.9957],\n",
      "        [442.3000,  69.2617,  88.5581,  26.5606],\n",
      "        [ 74.8486, 318.4805,  56.7456,  32.0555],\n",
      "        [159.4135, 274.5139,  99.0810,  20.1179],\n",
      "        [166.4519, 176.3331,  99.8266,  21.9709],\n",
      "        [168.7959, 327.4187,  89.9547,  23.0436],\n",
      "        [ 70.7483, 270.9219,  52.9756,  28.0602],\n",
      "        [546.6343,  69.4055,  92.2136,  27.1148],\n",
      "        [326.7038,  69.8137,  88.9962,  25.4324],\n",
      "        [497.1260, 563.1924, 112.4473,  28.9268],\n",
      "        [271.4084, 564.8047, 129.9357,  28.0305],\n",
      "        [116.2394, 524.2043, 157.4163,  21.6995],\n",
      "        [403.1331, 523.0760, 144.2040,  24.5954],\n",
      "        [132.9368,  69.9969, 139.0139,  23.5857]], device='cuda:0')\n",
      "xywhn: tensor([[0.1132, 0.2686, 0.0705, 0.0495],\n",
      "        [0.1089, 0.3422, 0.0671, 0.0467],\n",
      "        [0.2471, 0.3547, 0.1657, 0.0344],\n",
      "        [0.6911, 0.1082, 0.1384, 0.0415],\n",
      "        [0.1170, 0.4976, 0.0887, 0.0501],\n",
      "        [0.2491, 0.4289, 0.1548, 0.0314],\n",
      "        [0.2601, 0.2755, 0.1560, 0.0343],\n",
      "        [0.2637, 0.5116, 0.1406, 0.0360],\n",
      "        [0.1105, 0.4233, 0.0828, 0.0438],\n",
      "        [0.8541, 0.1084, 0.1441, 0.0424],\n",
      "        [0.5105, 0.1091, 0.1391, 0.0397],\n",
      "        [0.7768, 0.8800, 0.1757, 0.0452],\n",
      "        [0.4241, 0.8825, 0.2030, 0.0438],\n",
      "        [0.1816, 0.8191, 0.2460, 0.0339],\n",
      "        [0.6299, 0.8173, 0.2253, 0.0384],\n",
      "        [0.2077, 0.1094, 0.2172, 0.0369]], device='cuda:0')\n",
      "xyxy: tensor([[ 49.8921, 156.0654,  95.0249, 187.7275],\n",
      "        [ 48.2171, 204.0638,  91.1322, 233.9276],\n",
      "        [105.1133, 216.0390, 211.1543, 238.0346],\n",
      "        [398.0210,  55.9814, 486.5791,  82.5420],\n",
      "        [ 46.4758, 302.4528, 103.2214, 334.5083],\n",
      "        [109.8730, 264.4549, 208.9540, 284.5728],\n",
      "        [116.5386, 165.3476, 216.3652, 187.3185],\n",
      "        [123.8185, 315.8969, 213.7732, 338.9405],\n",
      "        [ 44.2604, 256.8918,  97.2361, 284.9520],\n",
      "        [500.5275,  55.8481, 592.7411,  82.9629],\n",
      "        [282.2057,  57.0975, 371.2019,  82.5299],\n",
      "        [440.9023, 548.7290, 553.3496, 577.6558],\n",
      "        [206.4406, 550.7894, 336.3762, 578.8199],\n",
      "        [ 37.5312, 513.3546, 194.9476, 535.0541],\n",
      "        [331.0311, 510.7784, 475.2351, 535.3738],\n",
      "        [ 63.4299,  58.2040, 202.4438,  81.7898]], device='cuda:0')\n",
      "xyxyn: tensor([[0.0780, 0.2439, 0.1485, 0.2933],\n",
      "        [0.0753, 0.3188, 0.1424, 0.3655],\n",
      "        [0.1642, 0.3376, 0.3299, 0.3719],\n",
      "        [0.6219, 0.0875, 0.7603, 0.1290],\n",
      "        [0.0726, 0.4726, 0.1613, 0.5227],\n",
      "        [0.1717, 0.4132, 0.3265, 0.4446],\n",
      "        [0.1821, 0.2584, 0.3381, 0.2927],\n",
      "        [0.1935, 0.4936, 0.3340, 0.5296],\n",
      "        [0.0692, 0.4014, 0.1519, 0.4452],\n",
      "        [0.7821, 0.0873, 0.9262, 0.1296],\n",
      "        [0.4409, 0.0892, 0.5800, 0.1290],\n",
      "        [0.6889, 0.8574, 0.8646, 0.9026],\n",
      "        [0.3226, 0.8606, 0.5256, 0.9044],\n",
      "        [0.0586, 0.8021, 0.3046, 0.8360],\n",
      "        [0.5172, 0.7981, 0.7426, 0.8365],\n",
      "        [0.0991, 0.0909, 0.3163, 0.1278]], device='cuda:0')\n",
      "{0: 'button', 1: 'checkbox', 2: 'container', 3: 'icon-button', 4: 'image', 5: 'input', 6: 'label', 7: 'link', 8: 'number-input', 9: 'radio', 10: 'search', 11: 'select', 12: 'slider', 13: 'table', 14: 'text', 15: 'textarea', 16: 'textbox', 17: 'toggle'}\n"
     ]
    }
   ],
   "source": [
    "dc.predict(\"/home/nysa/doodlecode/datasets/yolo-v8/test/images/12_jpeg.rf.09ef5ab9a0e4a83fd34e97dca3c0fc70.jpg\", ultralytics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53ae9691-9864-4cf4-96de-f0c89ed8c2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Class Mapping:  {0: 'button', 1: 'checkbox', 2: 'container', 3: 'icon-button', 4: 'image', 5: 'input', 6: 'label', 7: 'link', 8: 'number-input', 9: 'radio', 10: 'search', 11: 'select', 12: 'slider', 13: 'table', 14: 'text', 15: 'textarea', 16: 'textbox', 17: 'toggle', 18: 'pagination', 19: 'paragraph', 20: 'carousel', 21: 'heading'}\n",
      "Backbone:  yolo_v8_xs_backbone_coco\n",
      "Learning Rate:  0.01\n",
      "Number of Epochs:  10\n",
      "Split:  0.7\n",
      "Patience:  10\n",
      "Batch Size:  4\n",
      "Sorting data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/main/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/doodlecode/doodlecode.py:90\u001b[0m, in \u001b[0;36mDoodleCode.train_model\u001b[0;34m(self, backbone, lr, split, patience, epochs, batch_size, path, weights, ultralytics)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     backbone\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolo_v8_xs_backbone_coco\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     ultralytics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m ):\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ultralytics:\n\u001b[0;32m---> 90\u001b[0m         model, dt \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgl_class_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m            \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m         save_model(model\u001b[38;5;241m=\u001b[39mmodel, path\u001b[38;5;241m=\u001b[39mpath, time\u001b[38;5;241m=\u001b[39mdt)\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/doodlecode/train.py:180\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(class_mapping, backbone, lr, num_epochs, split, patience, batch_size, weights)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatience: \u001b[39m\u001b[38;5;124m\"\u001b[39m, patience)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Size: \u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size)\n\u001b[0;32m--> 180\u001b[0m image_paths, bbox, classes \u001b[38;5;241m=\u001b[39m \u001b[43msort_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m train_data, val_data \u001b[38;5;241m=\u001b[39m tf_data_gen(image_paths, bbox, classes, split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_data)\n",
      "File \u001b[0;32m~/doodlecode/train.py:79\u001b[0m, in \u001b[0;36msort_data\u001b[0;34m(class_mapping)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSorting data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Get all XML file paths in path_annot and sort them\u001b[39;00m\n\u001b[1;32m     76\u001b[0m xml_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m     77\u001b[0m     [\n\u001b[1;32m     78\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(gl_labels, file_name)\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgl_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m     ]\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Get all JPEG image file paths in path_images and sort them\u001b[39;00m\n\u001b[1;32m     85\u001b[0m jpg_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m     86\u001b[0m     [\n\u001b[1;32m     87\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(gl_labels, file_name)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m     ]\n\u001b[1;32m     91\u001b[0m )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/main/train'"
     ]
    }
   ],
   "source": [
    "dc.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5a3de-75e6-47b7-bb5b-e8ddd3b30440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
